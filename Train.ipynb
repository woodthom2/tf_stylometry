{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomas/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/thomas/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import re\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_folder = 'data/processed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_folder = 'output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_cnn import TextCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAMPLE_LENGTH = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dropout_keep_prob = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluate_every = 100\n",
    "checkpoint_every = 100\n",
    "num_checkpoints = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_dir = 'runs'\n",
    "num_checkpoints = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip, pickle as pkl\n",
    "with gzip.GzipFile(\"data/abridged_index2word.pkl.gz\", \"rb\") as f:\n",
    "    abridged_index2word = pkl.load(f)\n",
    "with gzip.GzipFile(\"data/abridged_word2index.pkl.gz\", \"rb\") as f:\n",
    "    abridged_word2index = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train_text = []\n",
    "for root, folder, files in os.walk(input_folder):\n",
    "    for file_name in files:\n",
    "        if file_name == \"README.md\":\n",
    "            continue\n",
    "        with gzip.open(input_folder + '/' + file_name, 'rb') as f:\n",
    "            all_token_ids = pkl.load(f)\n",
    "        cat = re.sub(\"_.+\", \"\", file_name)\n",
    "        X_train.append(all_token_ids)\n",
    "        y_train_text.append(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Anne': 0, 'Charlotte': 1, 'Emily': 2}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_cat = list(sorted(set(y_train_text)))\n",
    "cat_to_id = {}\n",
    "for idx, cat in enumerate(id_to_cat):\n",
    "    cat_to_id[cat] = idx\n",
    "cat_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Anne', 'Charlotte', 'Emily']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with gzip.GzipFile(\"data/id_to_cat.pkl.gz\", \"wb\") as f:\n",
    "    pkl.dump(id_to_cat, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 0, 0, 1, 2, 1]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = [cat_to_id[cat] for cat in y_train_text]\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn = TextCNN(\n",
    "            sequence_length=SAMPLE_LENGTH,\n",
    "            num_classes=len(id_to_cat),\n",
    "            vocab_size=len(abridged_index2word),\n",
    "            embedding_size=300,\n",
    "            filter_sizes=[3,4,5],\n",
    "            num_filters=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gensim_weights = [x for x in tf.global_variables() if \"embedding\" in str(x)]\n",
    "other_weights = [x for x in tf.global_variables() if \"embedding\" not in str(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver_embedding = tf.train.Saver(gensim_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n"
     ]
    }
   ],
   "source": [
    "# Keep track of gradient values and sparsity (optional)\n",
    "grad_summaries = []\n",
    "for g, v in grads_and_vars:\n",
    "    if g is not None:\n",
    "        grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "        sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "        grad_summaries.append(grad_hist_summary)\n",
    "        grad_summaries.append(sparsity_summary)\n",
    "grad_summaries_merged = tf.summary.merge(grad_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Summaries for loss and accuracy\n",
    "loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "#acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "saver = tf.train.Saver(other_weights, max_to_keep=num_checkpoints, keep_checkpoint_every_n_hours=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize all variables\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from output/gensim_weights\n"
     ]
    }
   ],
   "source": [
    "saver_embedding.restore(sess, model_folder + \"/gensim_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train Summaries\n",
    "train_summary_op = tf.summary.merge([loss_summary,\n",
    "                                     #acc_summary,\n",
    "                                     grad_summaries_merged])\n",
    "train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_step(x_batch, y_batch, f):\n",
    "    \"\"\"\n",
    "    A single training step\n",
    "    \"\"\"\n",
    "    feed_dict = {\n",
    "      cnn.input_x: x_batch,\n",
    "      cnn.input_y: y_batch,\n",
    "      cnn.dropout_keep_prob: dropout_keep_prob\n",
    "    }\n",
    "    '''\n",
    "    _, step, summaries, loss, accuracy = sess.run(\n",
    "        [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "        feed_dict)\n",
    "    '''\n",
    "    _, step, summaries, loss = sess.run(\n",
    "        [train_op, global_step, train_summary_op, cnn.loss],\n",
    "        feed_dict)\n",
    "    time_str = datetime.datetime.now().isoformat()\n",
    "    print(\"{}: step {}, loss {:g}, acc\".format(time_str, step, loss\n",
    "                                                    #, accuracy\n",
    "                                                   ))\n",
    "    train_summary_writer.add_summary(summaries, step)\n",
    "    f.write(time_str + \"\\t\" + str(step) + \"\\t\" + str(loss) + \"\\n\")\n",
    "    f.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 360,    2,    3, ...,   15, 1016,    0]),\n",
       " array([ 360,    2,    0, ...,   82,  278, 8120]),\n",
       " array([ 360,    2,    1, ...,   25,   38, 1363]),\n",
       " array([ 360,    2,    3, ..., 3118,    0,    0]),\n",
       " array([ 360,    2,    3, ...,    4,    1, 6687]),\n",
       " array([  3, 360,   2, ..., 284, 770,   3]),\n",
       " array([360,   2,   3, ...,   0,   0,   0])]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-28T12:20:27.814028: step 1, loss 3.16074, acc\n",
      "2018-05-28T12:20:27.962701: step 2, loss 1.96791, acc\n",
      "2018-05-28T12:20:28.107753: step 3, loss 0.961665, acc\n",
      "2018-05-28T12:20:28.249525: step 4, loss 7.27132, acc\n",
      "2018-05-28T12:20:28.398177: step 5, loss 0.108345, acc\n",
      "2018-05-28T12:20:28.553420: step 6, loss 5.58244, acc\n",
      "2018-05-28T12:20:28.722935: step 7, loss 3.80542, acc\n",
      "2018-05-28T12:20:28.895389: step 8, loss 3.79988, acc\n",
      "2018-05-28T12:20:29.040770: step 9, loss 4.02268, acc\n",
      "2018-05-28T12:20:29.182984: step 10, loss 3.81251, acc\n",
      "2018-05-28T12:20:29.326429: step 11, loss 0.662819, acc\n",
      "2018-05-28T12:20:29.464622: step 12, loss 0.95384, acc\n",
      "2018-05-28T12:20:29.612314: step 13, loss 1.16995, acc\n",
      "2018-05-28T12:20:29.758662: step 14, loss 1.63246, acc\n",
      "2018-05-28T12:20:29.904422: step 15, loss 0.892508, acc\n",
      "2018-05-28T12:20:30.050227: step 16, loss 2.18465, acc\n",
      "2018-05-28T12:20:30.199986: step 17, loss 2.67578, acc\n",
      "2018-05-28T12:20:30.349840: step 18, loss 2.55496, acc\n",
      "2018-05-28T12:20:30.495832: step 19, loss 2.36824, acc\n",
      "2018-05-28T12:20:30.633723: step 20, loss 1.96812, acc\n",
      "2018-05-28T12:20:30.775633: step 21, loss 1.21821, acc\n",
      "2018-05-28T12:20:30.914839: step 22, loss 1.45789, acc\n",
      "2018-05-28T12:20:31.064165: step 23, loss 2.8346, acc\n",
      "2018-05-28T12:20:31.229317: step 24, loss 1.3707, acc\n",
      "2018-05-28T12:20:31.378677: step 25, loss 1.00557, acc\n",
      "2018-05-28T12:20:31.546139: step 26, loss 0.71096, acc\n",
      "2018-05-28T12:20:31.696454: step 27, loss 1.86832, acc\n",
      "2018-05-28T12:20:31.838611: step 28, loss 0.33609, acc\n",
      "2018-05-28T12:20:31.992576: step 29, loss 0.752205, acc\n",
      "2018-05-28T12:20:32.149312: step 30, loss 0.265512, acc\n",
      "2018-05-28T12:20:32.308584: step 31, loss 3.44793, acc\n",
      "2018-05-28T12:20:32.460177: step 32, loss 0.434634, acc\n",
      "2018-05-28T12:20:32.622461: step 33, loss 1.7368, acc\n",
      "2018-05-28T12:20:32.764393: step 34, loss 5.10419, acc\n",
      "2018-05-28T12:20:32.916452: step 35, loss 0.681509, acc\n",
      "2018-05-28T12:20:33.052334: step 36, loss 1.74897, acc\n",
      "2018-05-28T12:20:33.200095: step 37, loss 1.72135, acc\n",
      "2018-05-28T12:20:33.342102: step 38, loss 0.329997, acc\n",
      "2018-05-28T12:20:33.485789: step 39, loss 1.11658, acc\n",
      "2018-05-28T12:20:33.623269: step 40, loss 3.60312, acc\n",
      "2018-05-28T12:20:33.772286: step 41, loss 1.07174, acc\n",
      "2018-05-28T12:20:33.911182: step 42, loss 0.728279, acc\n",
      "2018-05-28T12:20:34.055906: step 43, loss 0.935787, acc\n",
      "2018-05-28T12:20:34.199840: step 44, loss 1.88184, acc\n",
      "2018-05-28T12:20:34.346576: step 45, loss 0.702505, acc\n",
      "2018-05-28T12:20:34.486480: step 46, loss 1.14558, acc\n",
      "2018-05-28T12:20:34.630973: step 47, loss 1.55315, acc\n",
      "2018-05-28T12:20:34.773324: step 48, loss 0.447327, acc\n",
      "2018-05-28T12:20:34.916873: step 49, loss 3.53299, acc\n",
      "2018-05-28T12:20:35.051914: step 50, loss 1.06949, acc\n",
      "2018-05-28T12:20:35.201547: step 51, loss 0.510908, acc\n",
      "2018-05-28T12:20:35.339463: step 52, loss 1.0616, acc\n",
      "2018-05-28T12:20:35.483838: step 53, loss 0.778042, acc\n",
      "2018-05-28T12:20:35.623394: step 54, loss 0.71411, acc\n",
      "2018-05-28T12:20:35.763684: step 55, loss 2.44441, acc\n",
      "2018-05-28T12:20:35.904203: step 56, loss 1.64014, acc\n",
      "2018-05-28T12:20:36.044587: step 57, loss 0.678766, acc\n",
      "2018-05-28T12:20:36.180986: step 58, loss 0.542499, acc\n",
      "2018-05-28T12:20:36.327442: step 59, loss 1.68268, acc\n",
      "2018-05-28T12:20:36.465226: step 60, loss 0.608987, acc\n",
      "2018-05-28T12:20:36.613953: step 61, loss 0.755151, acc\n",
      "2018-05-28T12:20:36.753272: step 62, loss 2.25638, acc\n",
      "2018-05-28T12:20:36.898703: step 63, loss 1.53192, acc\n",
      "2018-05-28T12:20:37.036884: step 64, loss 2.26161, acc\n",
      "2018-05-28T12:20:37.181535: step 65, loss 0.522258, acc\n",
      "2018-05-28T12:20:37.319439: step 66, loss 1.92297, acc\n",
      "2018-05-28T12:20:37.465373: step 67, loss 0.84652, acc\n",
      "2018-05-28T12:20:37.622871: step 68, loss 1.07325, acc\n",
      "2018-05-28T12:20:37.782828: step 69, loss 0.510843, acc\n",
      "2018-05-28T12:20:37.929501: step 70, loss 1.93549, acc\n",
      "2018-05-28T12:20:38.079221: step 71, loss 2.00062, acc\n",
      "2018-05-28T12:20:38.227824: step 72, loss 0.898371, acc\n",
      "2018-05-28T12:20:38.386001: step 73, loss 1.89558, acc\n",
      "2018-05-28T12:20:38.554417: step 74, loss 1.49217, acc\n",
      "2018-05-28T12:20:38.720321: step 75, loss 1.2937, acc\n",
      "2018-05-28T12:20:38.912153: step 76, loss 0.902262, acc\n",
      "2018-05-28T12:20:39.092558: step 77, loss 3.47835, acc\n",
      "2018-05-28T12:20:39.264086: step 78, loss 0.833408, acc\n",
      "2018-05-28T12:20:39.420296: step 79, loss 2.59985, acc\n",
      "2018-05-28T12:20:39.581448: step 80, loss 0.295464, acc\n",
      "2018-05-28T12:20:39.738017: step 81, loss 3.43261, acc\n",
      "2018-05-28T12:20:39.886912: step 82, loss 1.41549, acc\n",
      "2018-05-28T12:20:40.035850: step 83, loss 0.597462, acc\n",
      "2018-05-28T12:20:40.177875: step 84, loss 0.359966, acc\n",
      "2018-05-28T12:20:40.324722: step 85, loss 0.57867, acc\n",
      "2018-05-28T12:20:40.467424: step 86, loss 1.26869, acc\n",
      "2018-05-28T12:20:40.625586: step 87, loss 0.0952602, acc\n",
      "2018-05-28T12:20:40.773088: step 88, loss 0.314494, acc\n",
      "2018-05-28T12:20:40.933841: step 89, loss 0.589857, acc\n",
      "2018-05-28T12:20:41.084785: step 90, loss 3.20932, acc\n",
      "2018-05-28T12:20:41.229737: step 91, loss 0.633807, acc\n",
      "2018-05-28T12:20:41.377706: step 92, loss 3.03193, acc\n",
      "2018-05-28T12:20:41.526848: step 93, loss 2.42779, acc\n",
      "2018-05-28T12:20:41.671199: step 94, loss 0.963397, acc\n",
      "2018-05-28T12:20:41.817901: step 95, loss 1.07258, acc\n",
      "2018-05-28T12:20:41.961127: step 96, loss 1.35634, acc\n",
      "2018-05-28T12:20:42.106928: step 97, loss 1.469, acc\n",
      "2018-05-28T12:20:42.262890: step 98, loss 0.367501, acc\n",
      "2018-05-28T12:20:42.417421: step 99, loss 1.73007, acc\n",
      "2018-05-28T12:20:42.566017: step 100, loss 1.08122, acc\n",
      "Saved model checkpoint to /media/thomas/026919b3-ea3e-4923-96aa-7f83aae1d652/stylometry/tf_stylometry/runs/checkpoints/model-100\n",
      "\n",
      "2018-05-28T12:20:42.747645: step 101, loss 0.939373, acc\n",
      "2018-05-28T12:20:42.890497: step 102, loss 1.14374, acc\n",
      "2018-05-28T12:20:43.037458: step 103, loss 1.91912, acc\n",
      "2018-05-28T12:20:43.176839: step 104, loss 0.458051, acc\n",
      "2018-05-28T12:20:43.327729: step 105, loss 0.952655, acc\n",
      "2018-05-28T12:20:43.473296: step 106, loss 0.643299, acc\n",
      "2018-05-28T12:20:43.621481: step 107, loss 1.02005, acc\n",
      "2018-05-28T12:20:43.762248: step 108, loss 2.03888, acc\n",
      "2018-05-28T12:20:43.910949: step 109, loss 0.198254, acc\n",
      "2018-05-28T12:20:44.065069: step 110, loss 2.29973, acc\n",
      "2018-05-28T12:20:44.234169: step 111, loss 0.263008, acc\n",
      "2018-05-28T12:20:44.385169: step 112, loss 0.453199, acc\n",
      "2018-05-28T12:20:44.564441: step 113, loss 0.577781, acc\n",
      "2018-05-28T12:20:44.726652: step 114, loss 0.457136, acc\n",
      "2018-05-28T12:20:44.887801: step 115, loss 0.410575, acc\n",
      "2018-05-28T12:20:45.027642: step 116, loss 1.5224, acc\n",
      "2018-05-28T12:20:45.174183: step 117, loss 3.4788, acc\n",
      "2018-05-28T12:20:45.315662: step 118, loss 1.94341, acc\n",
      "2018-05-28T12:20:45.470718: step 119, loss 0.666237, acc\n",
      "2018-05-28T12:20:45.639455: step 120, loss 0.490964, acc\n",
      "2018-05-28T12:20:45.796881: step 121, loss 1.16523, acc\n",
      "2018-05-28T12:20:45.955871: step 122, loss 0.821519, acc\n",
      "2018-05-28T12:20:46.106197: step 123, loss 1.61007, acc\n",
      "2018-05-28T12:20:46.254869: step 124, loss 2.79479, acc\n",
      "2018-05-28T12:20:46.412892: step 125, loss 0.27402, acc\n",
      "2018-05-28T12:20:46.604994: step 126, loss 1.03279, acc\n",
      "2018-05-28T12:20:46.808734: step 127, loss 1.15426, acc\n",
      "2018-05-28T12:20:46.980213: step 128, loss 0.712047, acc\n",
      "2018-05-28T12:20:47.128613: step 129, loss 0.954426, acc\n",
      "2018-05-28T12:20:47.273268: step 130, loss 0.886397, acc\n",
      "2018-05-28T12:20:47.427489: step 131, loss 0.512628, acc\n",
      "2018-05-28T12:20:47.579055: step 132, loss 0.68452, acc\n",
      "2018-05-28T12:20:47.725864: step 133, loss 2.86782, acc\n",
      "2018-05-28T12:20:47.872829: step 134, loss 0.805006, acc\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-bafd1cbd9422>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0my_data_single_segment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msegment\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_to_cat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0my_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my_data_single_segment\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mcurrent_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-caa389b66450>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(x_batch, y_batch, f)\u001b[0m\n\u001b[1;32m     15\u001b[0m     _, step, summaries, loss = sess.run(\n\u001b[1;32m     16\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_summary_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         feed_dict)\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mtime_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misoformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     print(\"{}: step {}, loss {:g}, acc\".format(time_str, step, loss\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open(model_folder + \"/loss.txt\", \"a\") as f:\n",
    "    for epoch in range(1000000):\n",
    "        segments = list(range(len(X_train)))\n",
    "        shuffle(segments)\n",
    "        \n",
    "        for segment in segments:\n",
    "            X_batch = X_train[segment]\n",
    "            start_poss = []\n",
    "            for i in range(BATCH_SIZE):\n",
    "                start_pos = randint(0, len(X_batch) - SAMPLE_LENGTH - 1)\n",
    "                start_poss.append(start_pos)\n",
    "            \n",
    "            X_data = [X_batch[start_pos:start_pos + SAMPLE_LENGTH] for start_pos in start_poss]\n",
    "            y_data_single_segment = [int(a == y_train[segment]) for a in range(len(id_to_cat))]\n",
    "            y_data = [y_data_single_segment for i in range(BATCH_SIZE)]\n",
    "            train_step(X_data, y_data, f)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "\n",
    "            if current_step % checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
